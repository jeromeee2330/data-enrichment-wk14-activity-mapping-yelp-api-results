














# Standard Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Additional Imports
import os, json, math, time
from yelpapi import YelpAPI
from tqdm.notebook import tqdm_notebook


!pip install yelpapi --quiet














# create a relative filepath
relative_path = os.path.join('.secret', 'yelp_api.json')


# Instantiate YelpAPI Variable
# Load API Credentials
# Correcting the file name in the path
with open(r'C:\Users\Lenovo\Documents\GitHub\dojo-env-setup\data-enrichment-wk14-activity-mapping-yelp-api-results\.secret\yelp_api.json') as f:
    login = json.load(f)

print(login.keys())





# set our API call parameters and filename before the first call
yelp_api = YelpAPI(login['api-key'], timeout_s = 5.0)
yelp_api


# set our API call parameters and filename before the first call
LOCATION = 'Greenville, SC'
TERM = 'Sushi'


## Specify fodler for saving data
folder_path = r"C:\Users\Lenovo\Documents\GitHub\dojo-env-setup\data-enrichment-wk14-activity-mapping-yelp-api-results"

# Specifying JSON_FILE filename (can include a folder)
JSON_FILE = f'/Data/results_SC_Sushi.json'





## Check if JSON_FILE exists
file_exists = os.path.isfile(JSON_FILE)

## If it does not exist:
if file_exists == False:
    
    ## CREATE ANY NEEDED FOLDERS
    # Get the Folder Name only
    folder = os.path.dirname(JSON_FILE)
    
    ## If JSON_FILE included a folder:
    if len(folder) > 0:
        # create the folder
        os.makedirs(folder, exist_ok = True)
        
    ## INFORM USER AND SAVE EMPTY LIST
    print(f'[i] {JSON_FILE} not found. Saving empty list to file.')
    
    ## save the first page of results
    with open(JSON_FILE, 'w') as file:
        json.dump([], file)
        
## If it exists, inform user
else:
    print(f'[i] {JSON_FILE} already exits.')





## Load previous results and use len of results for offset
with open(JSON_FILE, 'r') as file:
    previous_results = json.load(file)
    
## set offset based on previous results
n_results = len(previous_results)

print(f'- {n_results} previous results found.')









# use our yelp_api variable's search_query method to perform our API call
results = yelp_api.search_query(location = LOCATION,
                                term = TERM,
                                offset = n_results)

results.keys()


## How many results total?
total_results = results['total']
total_results





business_data = results['businesses']


# save the business data to a JSON file
#specify the filename where you want to save the data
json_file_path = JSON_FILE
with open(json_file_path, 'w') as file:
    json.dump(business_data, file, indent = 4)


## How many did we get the details for?
results_per_page = len(business_data)
print(f'number of results retrieved per page', results_per_page)





# Use math.ceil to round up for the total number of pages of results.
# Use math.ceil to round up for the total number of pages of results.
n_pages = math.ceil(total_results / results_per_page)

print(f'Total number of pages: {n_pages}')
# total number of api calls to make as to not exceed call limit
results_per_call = len(results['businesses'])


# Calculate the total number of iterations needed based on total results and results per call.
total_iterations = min(n_pages, math.ceil(total_results / results_per_call))
for i in tqdm_notebook(range(1, n_pages + 1)):

    ## The block of code we want to TRY to run
    try:        
        # Introduce a short delay to respect API rate limits
        time.sleep(0.2)
        
        ## Read in results in progress file and check the length
        with open(JSON_FILE, 'r') as file:
            previous_results = json.load(file)
        
        ## Save number of results to use as offset
        n_results = len(previous_results)
        
        ## Use n_results as the OFFSET 
        results = yelp_api.search_query(location = LOCATION,
                                        term = TERM,
                                        offset = len(previous_results))

        ## Append new results and save to file
        previous_results.extend(results['businesses'])
        with open(JSON_FILE, 'w') as file:
            json.dump(previous_results, file)
## What to do if we get an error/exception.
    except Exception as e:
        # check if we are at rate limit
        if 'Too Many Requests for url' in str(e):
            print('Rate limit exceeded. Stop data collection.')
            break
        else:
            print(f'an error occured {e}')
            # optionally handle error differently
            continue






df = pd.read_json(JSON_FILE)
display(df.head(), df.tail())


# check for duplicate IDs
df.duplicated(subset = 'id').sum()


## convert the filename to a .csv.gz
csv_file = JSON_FILE.replace('.json','.csv.gz')
csv_file


# specify directory and filename
directory = 'Data'
filename = 'final_results_SC_Sushi'
path = os.path.join(directory, filename)


# ensure that the 'Data' directory exists
os.makedirs(directory, exist_ok = True)


## Save it as a compressed csv (to save space)
df.to_csv(path, compression = 'gzip', index = False)
# Step 1: Correctly Save the JSON File
json_file = 'Data/final_results_SC_Sushi.json'  # Specify the correct JSON file name
os.makedirs('Data', exist_ok = True)  # Ensure the Data directory exists
df.to_json(json_file, orient = 'records', lines = True)  # Save the DataFrame as JSON

# Step 2: Convert and Save as .CSV.GZ
csv_gz_file = json_file.replace('.json', '.csv.gz')  # Create the CSV.GZ file name based on the JSON file name
df.to_csv(csv_gz_file, compression = 'gzip', index = False)  # Save the DataFrame as compressed CSV
# Step 3: Compare File Sizes
if os.path.exists(json_file) and os.path.exists(csv_gz_file):
    size_json = os.path.getsize(json_file)
    size_csv_gz = os.path.getsize(csv_gz_file)

    print(f'JSON FILE: {size_json:,} Bytes')
    print(f'CSV.GZ FILE: {size_csv_gz:,} Bytes')

    if size_csv_gz > 0:
        compression_ratio = size_json / size_csv_gz
        print(f'The csv.gz file is {compression_ratio:.2f} times smaller than the JSON file.')
    else:
        print("CSV.GZ file size is 0, cannot compare sizes.")
else:
    print("One or both files do not exist, check file paths.")






json_file = 'Data/final_results_SC_Sushi.json' 
size_json = os.path.getsize(JSON_FILE)
size_csv_gz = os.path.getsize(JSON_FILE.replace('.json','.csv.gz'))

print(f'JSON FILE: {size_json:,} Bytes')
print(f'CSV.GZ FILE: {size_csv_gz:,} Bytes')

print(f'the csv.gz is {size_json/size_csv_gz} times smaller!')



